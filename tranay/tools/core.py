# tranay/tools/core.py

import json
from typing import Annotated, Union
import pandas as pd
from pydantic import Field
import subprocess 
from tranay.tools import query_utils
from . import api_client, sumo_handler
import os

class Core:

    def __init__(self, data_sources): 
        self.data_sources = data_sources
        self.simulation_jobs = {}
        # The self.tools list now includes our new `list_api_projects` tool.
        self.tools = [
            self.list_sources,
            self.describe_table,
            self.run_query,
            self.list_projects,
            self.list_unique_values,

            self.create_sumo_configuration,
            self.start_simulation,
            self.check_simulation_status,
            self.load_simulation_results,
            self.get_bounding_box 
        ]
    
    def get_bounding_box(self, location: str) -> str:
        """
        Look up a place name via Nominatim and return a bare SUMO-ready bbox:
        'south,west,north,east'.
        """
        try:
            return sumo_handler.get_bbox_from_location_name(location)
        except Exception as e:
            # Propagate a clear error string back to the user
            return f"Error finding bounding box for '{location}': {e}"

    def create_sumo_configuration(self, 
        sim_name: Annotated[str, Field(description="A descriptive name for the simulation run, e.g., 'massy_morning_rush'.")],
        bbox: Annotated[str, Field(description="The geographic bounding box for the map data, in the format 'lat_min,lon_min,lat_max,lon_max'. For example: '48.7184,2.2268,48.7432,2.3142' for Massy, France.")]
    ) -> str:
        """
        Generates configuration files for a new SUMO simulation based on a real-world map area.
        Downloads map data from OpenStreetMap, converts it, and generates random traffic.
        Returns the path to the main .sumocfg file needed to start the simulation.
        """
        try:
            # Create a dedicated directory for this simulation's files
            sim_dir = os.path.join(os.getcwd(), "simulations", sim_name)

            # Call our new, more powerful handler function
            config_path = sumo_handler.create_scenario_from_bbox(
                sim_name=sim_name,
                sim_dir=sim_dir,
                bbox=bbox
            )

            return f"Successfully created configuration for '{sim_name}' in '{sim_dir}'. The config file path is: {config_path}"
        except Exception as e:
            return f"Error creating SUMO configuration: {e}"
        
    # def start_simulation(self, 
    #     config_file_path: Annotated[str, Field(description="The full path to the .sumocfg file generated by `create_sumo_configuration`.")],
    #     sim_name: Annotated[str, Field(description="The same descriptive name used when creating the configuration.")]
    # ) -> str:
    #     """
    #     Starts a SUMO simulation as a background job using a provided config file.
    #     Returns a unique job_id to track the simulation's status.
    #     """
    #     try:
    #         if not os.path.exists(config_file_path):
    #             return f"Error: Config file not found at '{config_file_path}'"
            
    #         from tranay.studio.app import celery_app 
    #         task = celery_app.send_task('tranay.studio.tasks.run_sumo_simulation', args=[config_file_path, sim_name])
    #         job_id = task.id
            
    #         # Store job info
    #         self.simulation_jobs[job_id] = {"status": "STARTED", "config_path": config_file_path, "sim_name": sim_name}
            
    #         return f"Simulation started successfully. Your job ID is: {job_id}"
    #     except Exception as e:
    #         return f"Error starting simulation: {e}"
    def start_simulation(
        self,
        config_file_path: Annotated[
            str | None,
            Field(
                default=None,
                description="Path to the .sumocfg produced by create_sumo_configuration",
            ),
        ] = None,
        config_file: Annotated[
            str | None,
            Field(
                default=None,
                description="Alias for config_file_path (for MCP compatibility)",
            ),
        ] = None,
        sim_name: Annotated[
            str,
            Field(description="The same sim_name used when creating the configuration"),
        ] = None,
        visualize: Annotated[
            bool,
            Field(
                default=False,
                description=(
                    "If True, launches sumo-gui locally for live visualization; "
                    "otherwise dispatches a headless Celery job."
                ),
            ),
        ] = False,
    ) -> str:
        """
        Starts a SUMO simulation.

        Arguments:
          - config_file_path / config_file: path to your .sumocfg file.
          - sim_name: the name you gave the simulation.
          - visualize: if True, opens SUMO GUI locally; else sends to Celery.

        Returns:
          - GUI PID message if visualize=True
          - Job ID message if visualize=False
        """
        # Choose the right config path
        cfg = config_file_path or config_file
        if not cfg:
            return "Error: no config_file_path or config_file provided"
        if not os.path.exists(cfg):
            return f"Error: Config file not found at '{cfg}'"

        # 1) Live GUI mode
        if visualize:
            try:
                gui_proc = subprocess.Popen(["sumo-gui", "-c", cfg, "--start"])
                return f"SUMO GUI launched (PID {gui_proc.pid}) for '{sim_name}'."
            except FileNotFoundError:
                return (
                    "Error: 'sumo-gui' not found on PATH. "
                    "Please install SUMO GUI or ensure it's in your PATH."
                )
            except Exception as e:
                return f"Error launching SUMO GUI: {e}"

        # 2) Background Celery mode
        try:
            # Defer import to avoid circular app/core import
            from tranay.studio.app import celery_app

            task = celery_app.send_task(
                "tranay.studio.tasks.run_sumo_simulation",
                args=[cfg, sim_name],
            )
            job_id = task.id
            # Track the job in memory
            self.simulation_jobs[job_id] = {
                "status": "STARTED",
                "config_path": cfg,
                "sim_name": sim_name,
            }
            return f"Simulation started successfully (job ID: {job_id})"
        except Exception as e:
            return f"Error starting simulation: {e}"
        
    def check_simulation_status(self, 
        job_id: Annotated[str, Field(description="The unique job ID returned by `start_simulation`.")]
    ) -> str:
        """Checks the status (e.g., PENDING, SUCCESS, FAILURE) of a running simulation job."""
        try:
            from tranay.studio.app import celery_app
            task_result = celery_app.AsyncResult(job_id)
            status = task_result.state
            
            response = f"Status for job {job_id}: {status}.\n"
            
            if task_result.failed():
                response += f"Error details: {task_result.info}"
            elif task_result.successful():
                response += "Simulation completed successfully. You can now load the results using `load_simulation_results`."
                # Store the result data
                self.simulation_jobs[job_id]['status'] = 'SUCCESS'
                self.simulation_jobs[job_id]['result'] = task_result.info
                
            return response
        except Exception as e:
            return f"Error checking status for job {job_id}: {e}"
        

    def load_simulation_results(self,
        job_id: Annotated[str, Field(description="The job ID of a successfully completed simulation.")]
    ) -> str:
        """
        For a completed job, this tool parses the XML output and loads it as a new, temporary data source
        that can be used by other analysis tools like `run_query` or `line_plot`.
        Returns the new source_id for the results.
        """
        try:
            job_info = self.simulation_jobs.get(job_id)
            if not job_info or job_info.get('status') != 'SUCCESS':
                return f"Error: Job {job_id} is not yet completed successfully. Please check its status first."

            result_data = job_info['result']
            output_dir = result_data['output_dir']
            sim_name = job_info['sim_name']
            
            tripinfo_file = os.path.join(output_dir, f"{sim_name}_tripinfo.xml")
            
            # Parse the XML to a list of dicts
            records = sumo_handler.parse_tripinfo_xml(tripinfo_file)
            if not records:
                return "No trip info found in the simulation output."
                
            # Convert to a pandas DataFrame and save as a temporary file
            df = pd.DataFrame(records)
            temp_parquet_path = os.path.join(output_dir, f"{sim_name}_results.parquet")
            df.to_parquet(temp_parquet_path)

            # Register this new parquet file as a data source
            source_id = f"sumo_{job_id[:8]}"
            self.data_sources[source_id] = {
                'source_type': 'parquet',
                'url': temp_parquet_path,
                'active': True
            }
            return f"Results loaded successfully. Use the new data source_id '{source_id}' with a table name of '{temp_parquet_path}' for analysis."

        except Exception as e:
            return f"Error loading results for job {job_id}: {e}"

    def list_sources(self) -> str:
        """
        List all available data sources and the tables/collections within them.
        Returns a list of unique source IDs to be used for other queries.
        """
        try:
            if not self.data_sources:
                return "No data sources available. Add data sources."
            
            result = "Available data sources:\n\n"
            for source_id, source_info in self.data_sources.items():
                tables = query_utils.list_tables(source_info)
                if isinstance(tables, list):
                    tables_str = ', '.join(tables) if tables else "No tables found"
                else:
                    tables_str = str(tables)
                result += f"- {source_id}\n  Has tables: {tables_str}\n"
                
            return result
        except Exception as e:
            return f"Error listing data sources: {e}"

    def describe_table(self, 
        source: Annotated[str, Field(description='The unique data source ID.')], 
        table: Annotated[str, Field(description='The table or collection name in the data source.')] # <-- RENAME this parameter
    ) -> str:
        """
        Lists columns and their types for a table or shows a sample document for a collection.
        """
        try:
            source_info = self.data_sources.get(source)
            if not source_info:
                return f"Source '{source}' Not Found"
            
            # --- IMPORTANT: Use the new parameter name here as well ---
            result = query_utils.describe_table(source_info, table)

            if isinstance(result, pd.DataFrame):
                 return result.to_markdown(index=False)
                
            return str(result)
        except Exception as e:
            return f"Error describing table: {e}"

    def list_api_projects(self,
        source: Annotated[str, Field(description="The unique ID of the tranay_api data source.")]
    ) -> str:
        """
        Lists all available projects from the specified tranay_api data source.
        Returns a table of project names and their corresponding IDs.
        """
        try:
            source_info = self.data_sources.get(source)
            if not source_info or source_info.get('source_type') != 'tranay_api':
                return f"Error: Source '{source}' is not a valid tranay_api source."

            projects = api_client.get_all_projects(source_info['url'])
            if not projects:
                return "No projects found for this API source."
            
            df = pd.DataFrame(projects)
            return df.to_markdown(index=False)
        except Exception as e:
            return f"Error listing API projects: {e}"

    def run_query(
        self,
        source: Annotated[
            str, Field(description="The unique data source ID from 'list_sources'.")
        ],  
        query: Annotated[
            str | None, Field(description="For SQL sources, the SQL query to run.")
        ] = None,
        collection: Annotated[
            str | None, Field(description="For MongoDB sources, the name of the collection to query.")
        ] = None,
        filter: Annotated[
            Union[str, dict, None], Field(description="For MongoDB, a JSON filter string or a dictionary.")
        ] = None,
        pipeline: Annotated[
            list | None, Field(description="For MongoDB aggregations, a list of pipeline stages.")
        ] = None,
        projection: Annotated[
            dict | None, Field(description="For MongoDB finds, a projection document.")
        ] = None,
        project_id: Annotated[
            str | None, Field(description="For tranay_api sources, the ID of the project to fetch.")
        ] = None,
        dataframe_query: Annotated[
            str | None, Field(description="A query string to filter data after it's fetched. E.g., 'sensor_id == 2007'.")
        ] = None,
        # --- NEW PARAMETER ---
        limit: Annotated[
            int | None, Field(description="Limit the number of rows returned after applying filters.")
        ] = None
    ) -> str:
        """
        Run a query against a specified data source.
        - For SQL, use 'query'.
        - For MongoDB, use 'collection' with 'filter', 'pipeline', or 'projection'.
        - For the tranay_api source, use 'project_id' and optionally 'dataframe_query'.
        - Use 'limit' to restrict the final output row count.
        """
        try:
            source_info = self.data_sources.get(source)
            if not source_info:
                return f"Source '{source}' Not Found"

            source_type = source_info['source_type']
            final_query_str = query_utils.build_query_str(
                source_info, query=query, collection=collection, 
                filter_obj=filter, pipeline=pipeline, projection=projection, project_id=project_id
            )

            if not final_query_str:
                return "Error: Could not build a valid query. Please provide correct parameters for the source type."

            result_df = query_utils.execute_query(source_info, final_query_str)

            if not isinstance(result_df, pd.DataFrame):
                return str(result_df) # Return error string directly

            # --- APPLY IN-MEMORY FILTERING ---
            if dataframe_query:
                try:
                    result_df = result_df.query(dataframe_query)
                    if result_df.empty:
                        return f"The dataframe_query '{dataframe_query}' resulted in no data."
                except Exception as e:
                    return f"Error applying dataframe_query: {e}"

            # --- APPLY ROW LIMIT ---
            if limit is not None:
                try:
                    result_df = result_df.head(limit)
                except Exception as e:
                    return f"Error applying limit: {e}"

            if result_df.empty:
                return "The query returned no results."

            return result_df.to_markdown(index=False)

        except Exception as e:
            return f"Error running query: {e}"

    def list_projects(self,
        source: Annotated[str, Field(description="The unique ID of the tranay_api data source.")]
    ) -> str:
        """
        Lists all available projects from the specified tranay_api data source.
        Returns a table of project names and their corresponding IDs.
        """
        try:
            source_info = self.data_sources.get(source)
            if not source_info or source_info.get('source_type') != 'tranay_api':
                return f"Error: Source '{source}' is not a valid tranay_api source."

            # The function now calls the api_client.get_all_projects
            projects = api_client.get_all_projects(source_info['url'])
            if not projects:
                return "No projects found for this API source."
            
            df = pd.DataFrame(projects)
            return df.to_markdown(index=False)
        except Exception as e:
            return f"Error listing API projects: {e}"

    def list_unique_values(
        self,
        source: Annotated[
            str, Field(description="The unique data source ID from 'list_sources'.")
        ],
        column: Annotated[
            str, Field(description="The column name to get unique values from.")
        ],
        project_id: Annotated[
            str | None, Field(description="For tranay_api sources, the ID of the project to fetch.")
        ] = None,
        dataframe_query: Annotated[
            str | None, Field(description="A query string to filter data before extracting unique values. E.g., 'speed > 10'.")
        ] = None,
        limit: Annotated[
            int | None, Field(description="Limit the number of unique values returned.")
        ] = None
) ->     str:
        """
        List unique values from a specified column in the data source.

        Args:
            source: The data source ID
            column: The column name to extract unique values from
            project_id: For tranay_api sources, the project ID to fetch
            dataframe_query: Optional query to filter data before extracting unique values
            limit: Optional limit on number of unique values returned

        Returns:
            String representation of unique values in markdown format
        """
        try:
            source_info = self.data_sources.get(source)
            if not source_info:
                return f"Source '{source}' Not Found"

            source_type = source_info['source_type']

            # Build query based on source type
            if source_type == 'tranay_api':
                if not project_id:
                    return "Error: project_id is required for tranay_api sources"
                final_query_str = query_utils.build_query_str(
                    source_info, project_id=project_id
                )
            else:
                # For other sources, you might want to build different queries
                final_query_str = query_utils.build_query_str(
                    source_info, collection=None, filter_obj=None, pipeline=None, projection=None
                )

            if not final_query_str:
                return "Error: Could not build a valid query. Please provide correct parameters for the source type."

            result_df = query_utils.execute_query(source_info, final_query_str)

            if not isinstance(result_df, pd.DataFrame):
                return str(result_df)  # Return error string directly

            # Apply dataframe query if provided
            if dataframe_query:
                try:
                    result_df = result_df.query(dataframe_query)
                    if result_df.empty:
                        return f"The dataframe_query '{dataframe_query}' resulted in no data."
                except Exception as e:
                    return f"Error applying dataframe_query: {e}"

            # Get unique values
            if column not in result_df.columns:
                return f"Error: Column '{column}' not found in the data. Available columns: {list(result_df.columns)}"

            unique_values = result_df[column].drop_duplicates().tolist()

            # Apply limit if specified
            if limit is not None:
                unique_values = unique_values[:limit]

            if not unique_values:
                return f"No unique values found in column '{column}'"

            # Create a simple dataframe for markdown output
            unique_df = pd.DataFrame({column: unique_values})
            return unique_df.to_markdown(index=False)

        except Exception as e:
            return f"Error listing unique values: {e}"